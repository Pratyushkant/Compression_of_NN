{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b54002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# ============================================================================\n",
    "# ITERATIVE LAYER-SENSITIVE COMPRESSION STRATEGY\n",
    "# Implementation of the multi-stage compression approach\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import time\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef9bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class LayerAnalysisCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN architecture for layer-wise compression analysis.\n",
    "    This model is designed with multiple convolutional and fully connected layers\n",
    "    to allow for individual layer compression.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(LayerAnalysisCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers (keep these unchanged)\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Only keep fc3, connecting directly from conv4 output\n",
    "        self.fc3 = nn.Linear(512 * 2 * 2, num_classes)  # Direct connection to output\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Remove fc1 and fc2 from layer_info\n",
    "        self.layer_info = OrderedDict([\n",
    "            ('conv1', {'layer': self.conv1, 'type': 'conv', 'position': 'early'}),\n",
    "            ('conv2', {'layer': self.conv2, 'type': 'conv', 'position': 'early'}),\n",
    "            ('conv3', {'layer': self.conv3, 'type': 'conv', 'position': 'middle'}),\n",
    "            ('conv4', {'layer': self.conv4, 'type': 'conv', 'position': 'middle'}),\n",
    "            ('fc3', {'layer': self.fc3, 'type': 'fc', 'position': 'final'})\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutional layers (unchanged)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "    \n",
    "        # Flatten for fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "    \n",
    "        # Direct connection to output layer\n",
    "        x = self.dropout(x)  # Optional: apply dropout before final layer\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaea5edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def algorithm_11_randomized_svd_power_iteration(A, k, q=2):\n",
    "    m, n = A.shape\n",
    "    Omega = np.random.randn(n, 2 * k)\n",
    "    Y = A @ Omega\n",
    "    for _ in range(q):\n",
    "        Y = A @ (A.T @ Y)\n",
    "    Q, R = np.linalg.qr(Y)\n",
    "    B = Q.T @ A\n",
    "    U_tilde, Sigma, Vt = np.linalg.svd(B, full_matrices=False)\n",
    "    U = Q @ U_tilde\n",
    "    rank = min(k, len(Sigma))\n",
    "    U = U[:, :rank]\n",
    "    Sigma = Sigma[:rank]\n",
    "    Vt = Vt[:rank, :]\n",
    "\n",
    "    return U, Sigma, Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754b985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def conv_to_matrix(weight, input_shape, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Converts a convolutional layer's weight tensor to its equivalent 2D matrix form\n",
    "    (also known as Toeplitz matrix or Im2Col matrix for convolution).\n",
    "    This is necessary for applying SVD, which operates on 2D matrices.\n",
    "\n",
    "    Args:\n",
    "        weight (torch.Tensor): The weight tensor of the convolutional layer\n",
    "                                (out_channels, in_channels, kh, kw).\n",
    "        input_shape (tuple): A dummy input shape (batch_size, channels, height, width)\n",
    "                             to correctly interpret the convolutional operation.\n",
    "        stride (int): Stride of the convolution.\n",
    "        padding (int): Padding of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reshaped weight matrix in NumPy format.\n",
    "    \"\"\"\n",
    "    # out_channels: number of output feature maps\n",
    "    # in_channels: number of input feature maps\n",
    "    # kh, kw: kernel height and width\n",
    "    out_channels, in_channels, kh, kw = weight.shape\n",
    "\n",
    "    # Reshape the weight tensor into a 2D matrix:\n",
    "    # (out_channels) x (in_channels * kh * kw)\n",
    "    # Each row corresponds to a single output filter.\n",
    "    # Each column corresponds to a flattened kernel element across all input channels.\n",
    "    weight_matrix = weight.view(out_channels, -1)\n",
    "\n",
    "    # Convert to NumPy array for SVD computation\n",
    "    return weight_matrix.cpu().numpy()\n",
    "\n",
    "def linear_to_matrix(weight):\n",
    "    \"\"\"\n",
    "    Converts a linear layer's weight tensor to its 2D matrix form.\n",
    "    For a linear layer, the weight is already in a 2D matrix form (out_features x in_features).\n",
    "\n",
    "    Args:\n",
    "        weight (torch.Tensor): The weight tensor of the linear layer\n",
    "                               (out_features, in_features).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The weight matrix in NumPy format.\n",
    "    \"\"\"\n",
    "    # Convert to NumPy array for SVD computation\n",
    "    return weight.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f83fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_parameters(model):\n",
    "    \"\"\"\n",
    "    Calculates the total number of trainable parameters in a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of parameters.\n",
    "    \"\"\"\n",
    "    # Sum the number of elements (numel) for all parameters in the model.\n",
    "    # This includes weights and biases for all layers.\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    \"\"\"\n",
    "    Evaluates the accuracy of a given PyTorch model on a dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to be evaluated.\n",
    "        dataloader (DataLoader): DataLoader for the dataset to evaluate on.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model in percentage.\n",
    "    \"\"\"\n",
    "    model.eval() # Set the model to evaluation mode (disables dropout, batch normalization updates, etc.)\n",
    "    correct, total = 0, 0 # Initialize counters for correct predictions and total samples\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient computation for evaluation (saves memory and speeds up)\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device) # Move data to device\n",
    "            outputs = model(inputs) # Perform forward pass\n",
    "            _, predicted = outputs.max(1) # Get the index of the max log-probability (the predicted class)\n",
    "            correct += predicted.eq(targets).sum().item() # Count correct predictions\n",
    "            total += targets.size(0) # Accumulate total samples\n",
    "\n",
    "    return 100. * correct / total # Calculate and return accuracy as a percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c2b0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, valloader, epochs=15, lr=0.001, patience=5):\n",
    "    \"\"\"\n",
    "    FIXED: Only use validation set for early stopping, never test set during training\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase - ONLY use validation set\n",
    "        val_acc = evaluate_model(model, valloader)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Print progress - REMOVED test accuracy during training\n",
    "        if epoch % 3 == 0:\n",
    "            print(f\"Epoch {epoch:2d}: Val Acc={val_acc:.2f}%\")\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}! No improvement for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d6a34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_compressed_model(model, trainloader, valloader, epochs=5, lr=0.0001, patience=3):\n",
    "    \"\"\"\n",
    "    FIXED: Removed testloader parameter - only use validation for early stopping\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation phase - ONLY use validation set\n",
    "        val_acc = evaluate_model(model, valloader)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return model  # Return model instead of test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16cd7e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class CompressedConv2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Compressed 2D Convolutional layer using low-rank decomposition (SVD).\n",
    "    This class replaces a standard Conv2d layer with two sequential convolutional layers\n",
    "    that approximate the original layer's operation using a lower rank.\n",
    "    \"\"\"\n",
    "    def __init__(self, U, S, Vt, original_shape, stride=1, padding=0, bias=None):\n",
    "        super(CompressedConv2D, self).__init__()\n",
    "\n",
    "        # original_shape: (out_channels, in_channels, kh, kw)\n",
    "        out_channels, in_channels, kh, kw = original_shape\n",
    "        rank = len(S) # The rank determined by SVD\n",
    "\n",
    "        self.original_shape = original_shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.rank = rank\n",
    "\n",
    "        # The low-rank approximation is achieved by two sequential conv layers:\n",
    "        # 1. conv1: Projects the input channels to 'rank' dimensions.\n",
    "        #    It takes 'in_channels' and produces 'rank' feature maps.\n",
    "        #    The kernel size is the original kernel size (kh, kw).\n",
    "        #    Bias is set to False as the original bias is applied in conv2.\n",
    "        self.conv1 = nn.Conv2d(in_channels, rank, kernel_size=(kh, kw),\n",
    "                              stride=1, padding=padding, bias=False)\n",
    "\n",
    "        # 2. conv2: Projects from 'rank' dimensions to 'out_channels'.\n",
    "        #    It takes 'rank' feature maps and produces 'out_channels'.\n",
    "        #    A 1x1 kernel is used for this recombination.\n",
    "        #    The original stride is applied here.\n",
    "        self.conv2 = nn.Conv2d(rank, out_channels, kernel_size=1,\n",
    "                              stride=stride, padding=0, bias=bias is not None)\n",
    "\n",
    "        # Initialize the weights of these two new layers using the SVD components\n",
    "        self._initialize_from_svd(U, S, Vt)\n",
    "\n",
    "        # If the original layer had a bias, apply it to the second convolutional layer\n",
    "        if bias is not None:\n",
    "            self.conv2.bias.data = bias\n",
    "\n",
    "    def _initialize_from_svd(self, U, S, Vt):\n",
    "        \"\"\"\n",
    "        Initializes the weights of conv1 and conv2 using the SVD components (U, S, Vt).\n",
    "        \"\"\"\n",
    "        rank = len(S)\n",
    "        out_channels, in_channels, kh, kw = self.original_shape\n",
    "\n",
    "        # The Vt matrix (rank x (in_channels * kh * kw)) is reshaped and assigned to conv1's weight.\n",
    "        # PyTorch Conv2d weights are (out_channels, in_channels, kh, kw).\n",
    "        # For conv1, out_channels is 'rank', in_channels is 'in_channels'.\n",
    "        V_reshaped = Vt.reshape(rank, in_channels, kh, kw)\n",
    "        self.conv1.weight.data = torch.from_numpy(V_reshaped).float()\n",
    "\n",
    "        # The U matrix (out_channels x rank) and singular values S (rank,) are combined\n",
    "        # (U @ diag(S)) and assigned to conv2's weight.\n",
    "        # PyTorch Conv2d weights for a 1x1 kernel are (out_channels, in_channels, 1, 1).\n",
    "        # For conv2, out_channels is 'out_channels', in_channels is 'rank'.\n",
    "        US = U @ np.diag(S)\n",
    "        self.conv2.weight.data = torch.from_numpy(US).float().unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the compressed convolutional layer.\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "# %%\n",
    "class CompressedLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Compressed Linear layer using low-rank decomposition (SVD).\n",
    "    This class replaces a standard Linear layer with two sequential linear layers\n",
    "    that approximate the original layer's operation using a lower rank.\n",
    "    \"\"\"\n",
    "    def __init__(self, U, S, Vt, original_shape, bias=None):\n",
    "        super(CompressedLinear, self).__init__()\n",
    "\n",
    "        # original_shape: (out_features, in_features)\n",
    "        out_features, in_features = original_shape\n",
    "        rank = len(S) # The rank determined by SVD\n",
    "\n",
    "        self.original_shape = original_shape\n",
    "        self.rank = rank\n",
    "\n",
    "        # The low-rank approximation is achieved by two sequential linear layers:\n",
    "        # 1. linear1: Projects the input features to 'rank' dimensions.\n",
    "        #    It takes 'in_features' and produces 'rank' features.\n",
    "        #    Bias is set to False as the original bias is applied in linear2.\n",
    "        self.linear1 = nn.Linear(in_features, rank, bias=False)\n",
    "\n",
    "        # 2. linear2: Projects from 'rank' dimensions to 'out_features'.\n",
    "        #    It takes 'rank' features and produces 'out_features'.\n",
    "        self.linear2 = nn.Linear(rank, out_features, bias=bias is not None)\n",
    "\n",
    "        # Initialize the weights of these two new layers using the SVD components\n",
    "        self._initialize_from_svd(U, S, Vt)\n",
    "\n",
    "        # If the original layer had a bias, apply it to the second linear layer\n",
    "        if bias is not None:\n",
    "            self.linear2.bias.data = bias\n",
    "\n",
    "    def _initialize_from_svd(self, U, S, Vt):\n",
    "        \"\"\"\n",
    "        Initializes the weights of linear1 and linear2 using the SVD components (U, S, Vt).\n",
    "        \"\"\"\n",
    "        # The Vt matrix (rank x in_features) is assigned to linear1's weight.\n",
    "        # PyTorch Linear weights are (out_features, in_features).\n",
    "        # For linear1, out_features is 'rank', in_features is 'in_features'.\n",
    "        self.linear1.weight.data = torch.from_numpy(Vt).float()\n",
    "\n",
    "        # The U matrix (out_features x rank) and singular values S (rank,) are combined\n",
    "        # (U @ diag(S)) and assigned to linear2's weight.\n",
    "        # PyTorch Linear weights are (out_features, in_features).\n",
    "        # For linear2, out_features is 'out_features', in_features is 'rank'.\n",
    "        US = U @ np.diag(S)\n",
    "        self.linear2.weight.data = torch.from_numpy(US).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the compressed linear layer.\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e93e96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_single_layer(model, layer_name, energy_retention, spectrum_analysis):\n",
    "    \"\"\"\n",
    "    Compresses a single layer to specified energy retention level.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to compress\n",
    "        layer_name: Name of the layer to compress\n",
    "        energy_retention: Target energy retention (0-1)\n",
    "        spectrum_analysis: Pre-computed singular value analysis\n",
    "        \n",
    "    Returns:\n",
    "        The model with the specified layer compressed\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    layer = getattr(model, layer_name)\n",
    "    weight = layer.weight.data\n",
    "    original_shape = weight.shape\n",
    "    \n",
    "    # Convert weight to matrix form\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        W_matrix = conv_to_matrix(weight, (1, weight.shape[1], 32, 32))\n",
    "        stride = layer.stride[0] if isinstance(layer.stride, tuple) else layer.stride\n",
    "        padding = layer.padding[0] if isinstance(layer.padding, tuple) else layer.padding\n",
    "    elif isinstance(layer, nn.Linear):\n",
    "        W_matrix = linear_to_matrix(weight)\n",
    "        stride, padding = None, None\n",
    "    else:\n",
    "        return model\n",
    "    \n",
    "    # Determine rank based on energy retention\n",
    "    if layer_name in spectrum_analysis:\n",
    "        energy_cumsum = spectrum_analysis[layer_name]['energy_cumsum']\n",
    "        rank = max(1, np.argmax(energy_cumsum >= energy_retention) + 1)\n",
    "        rank = min(rank, len(energy_cumsum))\n",
    "    else:\n",
    "        # Fallback: compute SVD on the fly\n",
    "        _, S_full, _ = np.linalg.svd(W_matrix, full_matrices=False)\n",
    "        energy_cumsum = np.cumsum(S_full**2) / np.sum(S_full**2)\n",
    "        rank = max(1, np.argmax(energy_cumsum >= energy_retention) + 1)\n",
    "        rank = min(rank, len(S_full))\n",
    "    \n",
    "    # Apply SVD compression\n",
    "    U, S, Vt = algorithm_11_randomized_svd_power_iteration(W_matrix, rank, q=2)\n",
    "    bias = layer.bias.data if layer.bias is not None else None\n",
    "    \n",
    "    # Create compressed layer\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        compressed_layer = CompressedConv2D(U, S, Vt, original_shape, stride, padding, bias)\n",
    "    else:\n",
    "        compressed_layer = CompressedLinear(U, S, Vt, original_shape, bias)\n",
    "    \n",
    "    compressed_layer = compressed_layer.to(device)\n",
    "    setattr(model, layer_name, compressed_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24bd1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar100_only(batch_size=128, validation_split=0.1):\n",
    "    \"\"\"\n",
    "    Loads and preprocesses the CIFAR-100 dataset with train/validation/test split.\n",
    "    \"\"\"\n",
    "    from torch.utils.data import random_split\n",
    "    \n",
    "    # Define transformations (same as before)\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # CIFAR-100 mean/std\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))  # CIFAR-100 mean/std\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR-100 instead of CIFAR-10\n",
    "    cifar100_train_full = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "    cifar100_test = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    # Split training data into train and validation\n",
    "    train_size = int((1 - validation_split) * len(cifar100_train_full))\n",
    "    val_size = len(cifar100_train_full) - train_size\n",
    "    cifar100_train, cifar100_val = random_split(cifar100_train_full, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoaders\n",
    "    datasets = {\n",
    "        'CIFAR-100': (  # Change key name\n",
    "            DataLoader(cifar100_train, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "            DataLoader(cifar100_val, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "            DataLoader(cifar100_test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1db9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_layer_sensitivity(model, spectrum_analysis, trainloader, valloader):\n",
    "    \"\"\"\n",
    "    FIXED: Use validation set instead of test set for sensitivity analysis\n",
    "    \"\"\"\n",
    "    print(\"Analyzing layer sensitivity to compression...\")\n",
    "    \n",
    "    sensitivity_results = {}\n",
    "    test_retention_levels = [0.99, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60]\n",
    "    \n",
    "    for layer_name in model.layer_info.keys():\n",
    "        print(f\"\\nTesting {layer_name}:\")\n",
    "        layer_results = []\n",
    "        \n",
    "        for retention in test_retention_levels:\n",
    "            # Test this layer at this retention level\n",
    "            test_model = copy.deepcopy(model)\n",
    "            test_model = compress_single_layer(test_model, layer_name, retention, spectrum_analysis)\n",
    "            \n",
    "            # Use VALIDATION set for evaluation during model selection\n",
    "            accuracy = evaluate_model(test_model, valloader)\n",
    "            \n",
    "            # Calculate compression ratio for this layer\n",
    "            original_layer = getattr(model, layer_name)\n",
    "            compressed_layer = getattr(test_model, layer_name)\n",
    "            original_params = original_layer.weight.numel()\n",
    "            compressed_params = calculate_total_parameters(compressed_layer)\n",
    "            compression_ratio = original_params / compressed_params\n",
    "            \n",
    "            layer_results.append({\n",
    "                'retention': retention,\n",
    "                'accuracy': accuracy,\n",
    "                'compression_ratio': compression_ratio\n",
    "            })\n",
    "            \n",
    "            print(f\"  {retention:.2f}: {accuracy:.2f}% ({compression_ratio:.2f}x)\")\n",
    "        \n",
    "        sensitivity_results[layer_name] = layer_results\n",
    "    \n",
    "    return sensitivity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea64004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_layer_compression(model, layer_name, spectrum_analysis, trainloader, valloader,\n",
    "                                 min_accuracy, min_retention=0.30, max_retention=0.99, tolerance=0.02):\n",
    "    \"\"\"\n",
    "    FIXED: Use validation set for compression optimization, not test set\n",
    "    \"\"\"\n",
    "    print(f\"  Finding optimal compression for {layer_name}...\")\n",
    "    \n",
    "    # First, do a coarse search to find the general range\n",
    "    coarse_retentions = [0.99, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.55, 0.50, 0.45, 0.40, 0.35, 0.30]\n",
    "    feasible_retentions = []\n",
    "    \n",
    "    for retention in coarse_retentions:\n",
    "        test_model = copy.deepcopy(model)\n",
    "        test_model = compress_single_layer(test_model, layer_name, retention, spectrum_analysis)\n",
    "        \n",
    "        # Fine-tune using only train/validation\n",
    "        fine_tune_compressed_model(test_model, trainloader, valloader, epochs=3, lr=0.0001, patience=2)\n",
    "        \n",
    "        # Evaluate on VALIDATION set\n",
    "        final_accuracy = evaluate_model(test_model, valloader)\n",
    "        \n",
    "        # Calculate compression ratio\n",
    "        original_layer = getattr(model, layer_name)\n",
    "        compressed_layer = getattr(test_model, layer_name)\n",
    "        original_params = original_layer.weight.numel()\n",
    "        compressed_params = calculate_total_parameters(compressed_layer)\n",
    "        compression_ratio = original_params / compressed_params\n",
    "        \n",
    "        print(f\"    Coarse search - Retention {retention:.2f}: Accuracy {final_accuracy:.2f}%, Ratio {compression_ratio:.2f}x\")\n",
    "        \n",
    "        if final_accuracy >= min_accuracy:\n",
    "            feasible_retentions.append((retention, final_accuracy, compression_ratio))\n",
    "    \n",
    "    if not feasible_retentions:\n",
    "        print(f\"    No feasible compression found for {layer_name}, using 0.99 retention\")\n",
    "        return 0.99, evaluate_model(model, valloader), 1.0\n",
    "    \n",
    "    # Find the most aggressive compression that still meets accuracy requirement\n",
    "    best_retention = min(feasible_retentions, key=lambda x: x[0])  # Minimum retention (most aggressive)\n",
    "    \n",
    "    return best_retention[0], best_retention[1], best_retention[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1381b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_iterative_compression(model, spectrum_analysis, trainloader, valloader, \n",
    "                                 max_accuracy_drop=1.0):\n",
    "    \"\"\"\n",
    "    FIXED: Use validation set for all compression decisions\n",
    "    MODIFIED: Apply fixed 95% energy retention for conv1 and 90% for fc3 at the beginning\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ADAPTIVE COMPRESSION - Max Accuracy Drop: {max_accuracy_drop:.1f}%\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get original VALIDATION accuracy (not test!)\n",
    "    original_accuracy = evaluate_model(model, valloader)\n",
    "    min_accuracy = original_accuracy - max_accuracy_drop\n",
    "    \n",
    "    print(f\"Original validation accuracy: {original_accuracy:.2f}%\")\n",
    "    print(f\"Minimum acceptable validation accuracy: {min_accuracy:.2f}%\")\n",
    "    \n",
    "    # Work on a copy of the model\n",
    "    compressed_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Define compression strategy based on accuracy drop threshold\n",
    "    if max_accuracy_drop <= 1.0:\n",
    "        layer_targets = {\n",
    "            'conv2': (0.90, 0.98),\n",
    "            'conv3': (0.85, 0.95),\n",
    "            'conv4': (0.80, 0.92)\n",
    "        }\n",
    "        print(\"Using CONSERVATIVE compression strategy\")\n",
    "    elif max_accuracy_drop <= 3.0:\n",
    "        layer_targets = {\n",
    "            'conv2': (0.75, 0.90),\n",
    "            'conv3': (0.70, 0.85),\n",
    "            'conv4': (0.65, 0.80)\n",
    "        }\n",
    "        print(\"Using MODERATE compression strategy\")\n",
    "    elif max_accuracy_drop <= 6.0:\n",
    "        layer_targets = {\n",
    "            'conv2': (0.60, 0.80),\n",
    "            'conv3': (0.55, 0.75),\n",
    "            'conv4': (0.50, 0.70)\n",
    "        }\n",
    "        print(\"Using AGGRESSIVE compression strategy\")\n",
    "    else:\n",
    "        layer_targets = {\n",
    "            'conv2': (0.45, 0.70),\n",
    "            'conv3': (0.40, 0.65),\n",
    "            'conv4': (0.35, 0.60)\n",
    "        }\n",
    "        print(\"Using VERY AGGRESSIVE compression strategy\")\n",
    "    \n",
    "    # Apply fixed compressions first\n",
    "    print(f\"\\nApplying fixed compressions:\")\n",
    "    print(f\"  conv1: 95% energy retention\")\n",
    "    print(f\"  fc3: 90% energy retention\")\n",
    "    \n",
    "    # Compress conv1 with 95% energy retention\n",
    "    compressed_model = compress_single_layer(compressed_model, 'conv1', 0.95, spectrum_analysis)\n",
    "    print(f\"  conv1 compressed with 95% energy retention\")\n",
    "    \n",
    "    # Compress fc3 with 90% energy retention  \n",
    "    compressed_model = compress_single_layer(compressed_model, 'fc3', 0.90, spectrum_analysis)\n",
    "    print(f\"  fc3 compressed with 90% energy retention\")\n",
    "    \n",
    "    # Fine-tune after fixed compressions\n",
    "    print(f\"  Fine-tuning model after fixed compressions...\")\n",
    "    fine_tune_compressed_model(compressed_model, trainloader, valloader, epochs=3, lr=0.0001, patience=2)\n",
    "    current_accuracy = evaluate_model(compressed_model, valloader)\n",
    "    print(f\"  Accuracy after fixed compressions: {current_accuracy:.2f}%\")\n",
    "    \n",
    "    # Calculate compression ratios for fixed layers\n",
    "    conv1_original = getattr(model, 'conv1')\n",
    "    conv1_compressed = getattr(compressed_model, 'conv1')\n",
    "    conv1_ratio = conv1_original.weight.numel() / calculate_total_parameters(conv1_compressed)\n",
    "    \n",
    "    fc3_original = getattr(model, 'fc3')\n",
    "    fc3_compressed = getattr(compressed_model, 'fc3')\n",
    "    fc3_ratio = fc3_original.weight.numel() / calculate_total_parameters(fc3_compressed)\n",
    "    \n",
    "    # Initialize layer_results with fixed compressions\n",
    "    layer_results = {\n",
    "        'conv1': {'energy_retention': 0.95, 'accuracy': current_accuracy, 'compression_ratio': conv1_ratio},\n",
    "        'fc3': {'energy_retention': 0.90, 'accuracy': current_accuracy, 'compression_ratio': fc3_ratio}\n",
    "    }\n",
    "    \n",
    "    # Continue adaptive compression for remaining layers\n",
    "    layer_order = ['conv4', 'conv3', 'conv2']  # Removed conv1 and fc3\n",
    "    \n",
    "    for layer_name in layer_order:\n",
    "        if layer_name not in layer_targets:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nCompressing {layer_name}...\")\n",
    "        print(f\"  Current model validation accuracy: {current_accuracy:.2f}%\")\n",
    "        \n",
    "        min_ret, max_ret = layer_targets[layer_name]\n",
    "        \n",
    "        # Find optimal compression for this layer using VALIDATION set\n",
    "        retention, accuracy, ratio = find_optimal_layer_compression(\n",
    "            compressed_model, layer_name, spectrum_analysis, trainloader, valloader,\n",
    "            min_accuracy, min_retention=min_ret, max_retention=max_ret\n",
    "        )\n",
    "        \n",
    "        # Apply compression\n",
    "        compressed_model = compress_single_layer(compressed_model, layer_name, retention, spectrum_analysis)\n",
    "        \n",
    "        # Fine-tune the entire model after compression\n",
    "        print(f\"  Fine-tuning model after {layer_name} compression...\")\n",
    "        fine_tune_compressed_model(compressed_model, trainloader, valloader, epochs=10, lr=0.0001, patience=3)\n",
    "        \n",
    "        # Evaluate on VALIDATION set\n",
    "        final_accuracy = evaluate_model(compressed_model, valloader)\n",
    "        \n",
    "        layer_results[layer_name] = {\n",
    "            'energy_retention': retention,\n",
    "            'accuracy': final_accuracy,\n",
    "            'compression_ratio': ratio\n",
    "        }\n",
    "        \n",
    "        current_accuracy = final_accuracy\n",
    "        print(f\"  {layer_name}: {retention:.3f} energy, {final_accuracy:.2f}% val accuracy, {ratio:.2f}x compression\")\n",
    "        \n",
    "        # Early stop if we're getting too close to the minimum accuracy\n",
    "        if final_accuracy < min_accuracy + 0.5:\n",
    "            print(f\"  Approaching minimum accuracy threshold, stopping compression\")\n",
    "            break\n",
    "    \n",
    "    # Calculate overall compression ratio\n",
    "    total_original_params = calculate_total_parameters(model)\n",
    "    total_compressed_params = calculate_total_parameters(compressed_model)\n",
    "    overall_compression_ratio = total_original_params / total_compressed_params\n",
    "    \n",
    "    # Final validation accuracy\n",
    "    final_accuracy = evaluate_model(compressed_model, valloader)\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"FINAL RESULTS (Validation Set):\")\n",
    "    print(f\"Original parameters: {total_original_params:,}\")\n",
    "    print(f\"Compressed parameters: {total_compressed_params:,}\")\n",
    "    print(f\"Overall compression ratio: {overall_compression_ratio:.2f}x\")\n",
    "    print(f\"Final validation accuracy: {final_accuracy:.2f}%\")\n",
    "    print(f\"Validation accuracy drop: {original_accuracy - final_accuracy:.2f}%\")\n",
    "    print(f\"Fixed compressions applied: conv1=95%, fc3=90%\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    return {\n",
    "        'model': compressed_model,\n",
    "        'original_accuracy': original_accuracy,\n",
    "        'final_accuracy': final_accuracy,\n",
    "        'accuracy_drop': original_accuracy - final_accuracy,\n",
    "        'overall_compression_ratio': overall_compression_ratio,\n",
    "        'total_original_params': total_original_params,\n",
    "        'total_compressed_params': total_compressed_params,\n",
    "        'layer_results': layer_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25c432fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def analyze_singular_value_spectrum(model, save_plots=True):\n",
    "    \"\"\"\n",
    "    Analyzes the singular value spectrum for each layer to understand \n",
    "    their decay characteristics and inherent low-rank nature.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained model to analyze\n",
    "        save_plots (bool): Whether to save spectrum plots\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results for each layer including decay rates and energy distributions\n",
    "    \"\"\"\n",
    "    spectrum_analysis = {}\n",
    "    \n",
    "    if save_plots:\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "        axes = axes.flatten()\n",
    "        plot_idx = 0\n",
    "    \n",
    "    for layer_name, info in model.layer_info.items():\n",
    "        layer = info['layer']\n",
    "        weight = layer.weight.data\n",
    "        \n",
    "        # Convert to matrix form for SVD\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            W_matrix = conv_to_matrix(weight, (1, weight.shape[1], 32, 32))\n",
    "        elif isinstance(layer, nn.Linear):\n",
    "            W_matrix = linear_to_matrix(weight)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Compute full SVD\n",
    "        _, S, _ = np.linalg.svd(W_matrix, full_matrices=False)\n",
    "        \n",
    "        # Normalize singular values\n",
    "        S_normalized = S / S[0]\n",
    "        \n",
    "        # Calculate energy retention curve\n",
    "        energy_cumsum = np.cumsum(S**2) / np.sum(S**2)\n",
    "        \n",
    "        # Analyze decay characteristics\n",
    "        # Find rank for common energy retention levels\n",
    "        ranks_for_energy = {}\n",
    "        for energy_level in [0.8, 0.85, 0.9, 0.95, 0.99]:\n",
    "            rank = np.argmax(energy_cumsum >= energy_level) + 1\n",
    "            ranks_for_energy[energy_level] = min(rank, len(S))\n",
    "        \n",
    "        # Calculate decay rate (exponential fit to first 20% of singular values)\n",
    "        n_fit = max(5, len(S) // 5)\n",
    "        log_s = np.log(S_normalized[:n_fit] + 1e-10)\n",
    "        indices = np.arange(n_fit)\n",
    "        decay_rate = -np.polyfit(indices, log_s, 1)[0]\n",
    "        \n",
    "        spectrum_analysis[layer_name] = {\n",
    "            'singular_values': S,\n",
    "            'energy_cumsum': energy_cumsum,\n",
    "            'ranks_for_energy': ranks_for_energy,\n",
    "            'decay_rate': decay_rate,\n",
    "            'effective_rank': np.sum(S > 0.01 * S[0]),  # Rank at 1% of max singular value\n",
    "            'layer_type': info['type'],\n",
    "            'layer_position': info['position']\n",
    "        }\n",
    "        \n",
    "        # Plot spectrum if requested\n",
    "        if save_plots and plot_idx < 8:\n",
    "            ax = axes[plot_idx]\n",
    "            ax.semilogy(S_normalized, 'b-', linewidth=2)\n",
    "            ax.set_title(f'{layer_name} - Decay Rate: {decay_rate:.3f}')\n",
    "            ax.set_xlabel('Singular Value Index')\n",
    "            ax.set_ylabel('Normalized Singular Value')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plot_idx += 1\n",
    "    \n",
    "    if save_plots:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('singular_value_spectra.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    return spectrum_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9393ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_improved_compression_experiments():\n",
    "    \"\"\"\n",
    "    FIXED: Separate test evaluation from model selection process\n",
    "    \"\"\"\n",
    "    print(\"Loading CIFAR-100 dataset with validation split...\")\n",
    "    datasets = load_cifar100_only(batch_size=128, validation_split=0.1)\n",
    "    trainloader, valloader, testloader = datasets['CIFAR-100']\n",
    "    \n",
    "    print(f\"Dataset sizes:\")\n",
    "    print(f\"  Training: {len(trainloader.dataset):,} samples\")\n",
    "    print(f\"  Validation: {len(valloader.dataset):,} samples\") \n",
    "    print(f\"  Test: {len(testloader.dataset):,} samples\")\n",
    "    \n",
    "    print(\"Creating and training model...\")\n",
    "    model = LayerAnalysisCNN(num_classes=100)\n",
    "    trained_model, best_val_accuracy = train_model(model, trainloader, valloader, \n",
    "                                                  epochs=60, lr=0.001, patience=5)\n",
    "    \n",
    "    print(f\"\\nModel training completed. Best validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    # ONLY evaluate on test set ONCE at the end for baseline\n",
    "    baseline_test_accuracy = evaluate_model(trained_model, testloader)\n",
    "    print(f\"Baseline test accuracy: {baseline_test_accuracy:.2f}%\")\n",
    "    print(f\"Total parameters: {calculate_total_parameters(trained_model):,}\")\n",
    "    \n",
    "    print(\"\\nAnalyzing singular value spectrum...\")\n",
    "    spectrum_analysis = analyze_singular_value_spectrum(trained_model, save_plots=False)\n",
    "    \n",
    "    # Test different accuracy drop thresholds using VALIDATION set\n",
    "    accuracy_drops = [0, 1.0, 3.0, 4.0, 5.0]\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for acc_drop in accuracy_drops:\n",
    "        print(f\"\\n\" + \"=\"*80)\n",
    "        print(f\"TESTING ACCURACY DROP THRESHOLD: {acc_drop}%\")\n",
    "        print(f\"=\"*80)\n",
    "        \n",
    "        try:\n",
    "            # Use validation set for compression decisions\n",
    "            results = adaptive_iterative_compression(\n",
    "                trained_model, spectrum_analysis, trainloader, valloader, \n",
    "                max_accuracy_drop=acc_drop\n",
    "            )\n",
    "            \n",
    "            # ONLY NOW evaluate final compressed model on test set\n",
    "            final_test_accuracy = evaluate_model(results['model'], testloader)\n",
    "            \n",
    "            # Store results for summary\n",
    "            all_results.append({\n",
    "                'max_accuracy_drop': acc_drop,\n",
    "                'original_val_accuracy': results['original_accuracy'],\n",
    "                'final_val_accuracy': results['final_accuracy'],\n",
    "                'final_test_accuracy': final_test_accuracy,  # NEW: separate test accuracy\n",
    "                'val_accuracy_drop': results['accuracy_drop'],\n",
    "                'test_accuracy_drop': baseline_test_accuracy - final_test_accuracy,  # NEW\n",
    "                'compression_ratio': results['overall_compression_ratio'],\n",
    "                'original_params': results['total_original_params'],\n",
    "                'compressed_params': results['total_compressed_params']\n",
    "            })\n",
    "            \n",
    "            print(f\"Final test accuracy for this compression: {final_test_accuracy:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with accuracy drop {acc_drop}%: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create summary table\n",
    "    print(f\"\\n\" + \"=\"*120)\n",
    "    print(\"FIXED COMPRESSION EXPERIMENT SUMMARY (No Data Leakage)\")\n",
    "    print(f\"=\"*120)\n",
    "    print(f\"{'Acc Drop':>8} {'Val Orig':>8} {'Val Final':>9} {'Test Final':>10} {'Val Drop':>8} {'Test Drop':>9} {'Ratio':>8} {'Orig Params':>12} {'Comp Params':>12}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    for result in all_results:\n",
    "        print(f\"{result['max_accuracy_drop']:>8.1f} \"\n",
    "              f\"{result['original_val_accuracy']:>8.2f} \"\n",
    "              f\"{result['final_val_accuracy']:>9.2f} \"\n",
    "              f\"{result['final_test_accuracy']:>10.2f} \"\n",
    "              f\"{result['val_accuracy_drop']:>8.2f} \"\n",
    "              f\"{result['test_accuracy_drop']:>9.2f} \"\n",
    "              f\"{result['compression_ratio']:>8.2f} \"\n",
    "              f\"{result['original_params']:>12,} \"\n",
    "              f\"{result['compressed_params']:>12,}\")\n",
    "    \n",
    "    print(f\"\\nBaseline test accuracy: {baseline_test_accuracy:.2f}%\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49a5f29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-100 dataset with validation split...\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset sizes:\n",
      "  Training: 45,000 samples\n",
      "  Validation: 5,000 samples\n",
      "  Test: 10,000 samples\n",
      "Creating and training model...\n",
      "Epoch  0: Val Acc=17.04%\n",
      "Epoch  3: Val Acc=33.18%\n",
      "Epoch  6: Val Acc=40.38%\n",
      "Epoch  9: Val Acc=44.50%\n",
      "Epoch 12: Val Acc=47.28%\n",
      "Epoch 15: Val Acc=48.06%\n",
      "Epoch 18: Val Acc=48.30%\n",
      "Epoch 21: Val Acc=51.50%\n",
      "Epoch 24: Val Acc=51.62%\n",
      "Epoch 27: Val Acc=51.26%\n",
      "Early stopping at epoch 29! No improvement for 5 epochs.\n",
      "\n",
      "Model training completed. Best validation accuracy: 51.62%\n",
      "Baseline test accuracy: 52.46%\n",
      "Total parameters: 1,755,876\n",
      "\n",
      "Analyzing singular value spectrum...\n",
      "\n",
      "================================================================================\n",
      "TESTING ACCURACY DROP THRESHOLD: 0%\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ADAPTIVE COMPRESSION - Max Accuracy Drop: 0.0%\n",
      "============================================================\n",
      "Original validation accuracy: 51.06%\n",
      "Minimum acceptable validation accuracy: 51.06%\n",
      "Using CONSERVATIVE compression strategy\n",
      "\n",
      "Applying fixed compressions:\n",
      "  conv1: 95% energy retention\n",
      "  fc3: 90% energy retention\n",
      "  conv1 compressed with 95% energy retention\n",
      "  fc3 compressed with 90% energy retention\n",
      "  Fine-tuning model after fixed compressions...\n",
      "  Accuracy after fixed compressions: 53.38%\n",
      "\n",
      "Compressing conv4...\n",
      "  Current model validation accuracy: 53.38%\n",
      "  Finding optimal compression for conv4...\n",
      "    Coarse search - Retention 0.99: Accuracy 53.22%, Ratio 1.03x\n",
      "    Coarse search - Retention 0.95: Accuracy 52.66%, Ratio 1.32x\n",
      "    Coarse search - Retention 0.90: Accuracy 53.08%, Ratio 1.64x\n",
      "    Coarse search - Retention 0.85: Accuracy 52.76%, Ratio 1.97x\n",
      "    Coarse search - Retention 0.80: Accuracy 52.74%, Ratio 2.34x\n",
      "    Coarse search - Retention 0.75: Accuracy 53.48%, Ratio 2.75x\n",
      "    Coarse search - Retention 0.70: Accuracy 51.92%, Ratio 3.24x\n",
      "    Coarse search - Retention 0.65: Accuracy 52.32%, Ratio 3.84x\n",
      "    Coarse search - Retention 0.60: Accuracy 51.74%, Ratio 4.54x\n",
      "    Coarse search - Retention 0.55: Accuracy 51.14%, Ratio 5.43x\n",
      "    Coarse search - Retention 0.50: Accuracy 50.90%, Ratio 6.63x\n",
      "    Coarse search - Retention 0.45: Accuracy 50.92%, Ratio 8.03x\n",
      "    Coarse search - Retention 0.40: Accuracy 50.68%, Ratio 10.17x\n",
      "    Coarse search - Retention 0.35: Accuracy 49.36%, Ratio 13.02x\n",
      "    Coarse search - Retention 0.30: Accuracy 47.22%, Ratio 17.32x\n",
      "  Fine-tuning model after conv4 compression...\n",
      "  conv4: 0.550 energy, 52.66% val accuracy, 5.43x compression\n",
      "\n",
      "Compressing conv3...\n",
      "  Current model validation accuracy: 52.66%\n",
      "  Finding optimal compression for conv3...\n",
      "    Coarse search - Retention 0.99: Accuracy 53.42%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 53.14%, Ratio 1.37x\n",
      "    Coarse search - Retention 0.90: Accuracy 52.90%, Ratio 1.95x\n",
      "    Coarse search - Retention 0.85: Accuracy 52.48%, Ratio 2.49x\n",
      "    Coarse search - Retention 0.80: Accuracy 52.50%, Ratio 3.07x\n",
      "    Coarse search - Retention 0.75: Accuracy 52.76%, Ratio 3.73x\n",
      "    Coarse search - Retention 0.70: Accuracy 52.60%, Ratio 4.44x\n",
      "    Coarse search - Retention 0.65: Accuracy 52.22%, Ratio 5.35x\n",
      "    Coarse search - Retention 0.60: Accuracy 51.22%, Ratio 6.51x\n",
      "    Coarse search - Retention 0.55: Accuracy 51.32%, Ratio 7.71x\n",
      "    Coarse search - Retention 0.50: Accuracy 50.20%, Ratio 9.44x\n",
      "    Coarse search - Retention 0.45: Accuracy 48.98%, Ratio 11.52x\n",
      "    Coarse search - Retention 0.40: Accuracy 46.50%, Ratio 14.77x\n",
      "    Coarse search - Retention 0.35: Accuracy 44.50%, Ratio 18.73x\n",
      "    Coarse search - Retention 0.30: Accuracy 41.80%, Ratio 22.81x\n",
      "  Fine-tuning model after conv3 compression...\n",
      "  conv3: 0.550 energy, 52.40% val accuracy, 7.71x compression\n",
      "\n",
      "Compressing conv2...\n",
      "  Current model validation accuracy: 52.40%\n",
      "  Finding optimal compression for conv2...\n",
      "    Coarse search - Retention 0.99: Accuracy 51.84%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 52.08%, Ratio 1.36x\n",
      "    Coarse search - Retention 0.90: Accuracy 52.04%, Ratio 2.05x\n",
      "    Coarse search - Retention 0.85: Accuracy 51.68%, Ratio 2.74x\n",
      "    Coarse search - Retention 0.80: Accuracy 51.54%, Ratio 3.36x\n",
      "    Coarse search - Retention 0.75: Accuracy 51.56%, Ratio 4.00x\n",
      "    Coarse search - Retention 0.70: Accuracy 51.46%, Ratio 4.72x\n",
      "    Coarse search - Retention 0.65: Accuracy 49.74%, Ratio 5.76x\n",
      "    Coarse search - Retention 0.60: Accuracy 49.66%, Ratio 6.90x\n",
      "    Coarse search - Retention 0.55: Accuracy 48.64%, Ratio 7.94x\n",
      "    Coarse search - Retention 0.50: Accuracy 48.42%, Ratio 9.37x\n",
      "    Coarse search - Retention 0.45: Accuracy 45.78%, Ratio 11.41x\n",
      "    Coarse search - Retention 0.40: Accuracy 44.34%, Ratio 12.80x\n",
      "    Coarse search - Retention 0.35: Accuracy 40.90%, Ratio 16.94x\n",
      "    Coarse search - Retention 0.30: Accuracy 36.02%, Ratio 20.21x\n",
      "  Fine-tuning model after conv2 compression...\n",
      "  conv2: 0.700 energy, 51.40% val accuracy, 4.72x compression\n",
      "  Approaching minimum accuracy threshold, stopping compression\n",
      "\n",
      "========================================\n",
      "FINAL RESULTS (Validation Set):\n",
      "Original parameters: 1,755,876\n",
      "Compressed parameters: 421,519\n",
      "Overall compression ratio: 4.17x\n",
      "Final validation accuracy: 52.18%\n",
      "Validation accuracy drop: -1.12%\n",
      "Fixed compressions applied: conv1=95%, fc3=90%\n",
      "========================================\n",
      "Final test accuracy for this compression: 53.05%\n",
      "\n",
      "================================================================================\n",
      "TESTING ACCURACY DROP THRESHOLD: 1.0%\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ADAPTIVE COMPRESSION - Max Accuracy Drop: 1.0%\n",
      "============================================================\n",
      "Original validation accuracy: 51.46%\n",
      "Minimum acceptable validation accuracy: 50.46%\n",
      "Using CONSERVATIVE compression strategy\n",
      "\n",
      "Applying fixed compressions:\n",
      "  conv1: 95% energy retention\n",
      "  fc3: 90% energy retention\n",
      "  conv1 compressed with 95% energy retention\n",
      "  fc3 compressed with 90% energy retention\n",
      "  Fine-tuning model after fixed compressions...\n",
      "  Accuracy after fixed compressions: 52.64%\n",
      "\n",
      "Compressing conv4...\n",
      "  Current model validation accuracy: 52.64%\n",
      "  Finding optimal compression for conv4...\n",
      "    Coarse search - Retention 0.99: Accuracy 53.50%, Ratio 1.03x\n",
      "    Coarse search - Retention 0.95: Accuracy 52.78%, Ratio 1.32x\n",
      "    Coarse search - Retention 0.90: Accuracy 53.44%, Ratio 1.64x\n",
      "    Coarse search - Retention 0.85: Accuracy 52.74%, Ratio 1.97x\n",
      "    Coarse search - Retention 0.80: Accuracy 54.34%, Ratio 2.34x\n",
      "    Coarse search - Retention 0.75: Accuracy 53.70%, Ratio 2.75x\n",
      "    Coarse search - Retention 0.70: Accuracy 52.18%, Ratio 3.24x\n",
      "    Coarse search - Retention 0.65: Accuracy 51.80%, Ratio 3.84x\n",
      "    Coarse search - Retention 0.60: Accuracy 51.92%, Ratio 4.54x\n",
      "    Coarse search - Retention 0.55: Accuracy 51.06%, Ratio 5.43x\n",
      "    Coarse search - Retention 0.50: Accuracy 51.32%, Ratio 6.63x\n",
      "    Coarse search - Retention 0.45: Accuracy 50.60%, Ratio 8.03x\n",
      "    Coarse search - Retention 0.40: Accuracy 49.62%, Ratio 10.17x\n",
      "    Coarse search - Retention 0.35: Accuracy 47.44%, Ratio 13.02x\n",
      "    Coarse search - Retention 0.30: Accuracy 47.98%, Ratio 17.32x\n",
      "  Fine-tuning model after conv4 compression...\n",
      "  conv4: 0.450 energy, 51.08% val accuracy, 8.03x compression\n",
      "\n",
      "Compressing conv3...\n",
      "  Current model validation accuracy: 51.08%\n",
      "  Finding optimal compression for conv3...\n",
      "    Coarse search - Retention 0.99: Accuracy 52.66%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 52.06%, Ratio 1.37x\n",
      "    Coarse search - Retention 0.90: Accuracy 52.40%, Ratio 1.95x\n",
      "    Coarse search - Retention 0.85: Accuracy 52.26%, Ratio 2.49x\n",
      "    Coarse search - Retention 0.80: Accuracy 51.04%, Ratio 3.07x\n",
      "    Coarse search - Retention 0.75: Accuracy 51.56%, Ratio 3.73x\n",
      "    Coarse search - Retention 0.70: Accuracy 50.90%, Ratio 4.44x\n",
      "    Coarse search - Retention 0.65: Accuracy 50.94%, Ratio 5.35x\n",
      "    Coarse search - Retention 0.60: Accuracy 50.16%, Ratio 6.51x\n",
      "    Coarse search - Retention 0.55: Accuracy 49.96%, Ratio 7.71x\n",
      "    Coarse search - Retention 0.50: Accuracy 47.72%, Ratio 9.44x\n",
      "    Coarse search - Retention 0.45: Accuracy 48.10%, Ratio 11.52x\n",
      "    Coarse search - Retention 0.40: Accuracy 45.64%, Ratio 14.77x\n",
      "    Coarse search - Retention 0.35: Accuracy 43.56%, Ratio 18.73x\n",
      "    Coarse search - Retention 0.30: Accuracy 41.34%, Ratio 22.81x\n",
      "  Fine-tuning model after conv3 compression...\n",
      "  conv3: 0.650 energy, 51.58% val accuracy, 5.35x compression\n",
      "\n",
      "Compressing conv2...\n",
      "  Current model validation accuracy: 51.58%\n",
      "  Finding optimal compression for conv2...\n",
      "    Coarse search - Retention 0.99: Accuracy 52.74%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 52.66%, Ratio 1.36x\n",
      "    Coarse search - Retention 0.90: Accuracy 51.48%, Ratio 2.05x\n",
      "    Coarse search - Retention 0.85: Accuracy 52.44%, Ratio 2.74x\n",
      "    Coarse search - Retention 0.80: Accuracy 52.00%, Ratio 3.36x\n",
      "    Coarse search - Retention 0.75: Accuracy 51.92%, Ratio 4.00x\n",
      "    Coarse search - Retention 0.70: Accuracy 51.50%, Ratio 4.72x\n",
      "    Coarse search - Retention 0.65: Accuracy 50.86%, Ratio 5.76x\n",
      "    Coarse search - Retention 0.60: Accuracy 49.54%, Ratio 6.90x\n",
      "    Coarse search - Retention 0.55: Accuracy 49.12%, Ratio 7.94x\n",
      "    Coarse search - Retention 0.50: Accuracy 47.38%, Ratio 9.37x\n",
      "    Coarse search - Retention 0.45: Accuracy 46.28%, Ratio 11.41x\n",
      "    Coarse search - Retention 0.40: Accuracy 44.14%, Ratio 12.80x\n",
      "    Coarse search - Retention 0.35: Accuracy 40.88%, Ratio 16.94x\n",
      "    Coarse search - Retention 0.30: Accuracy 36.12%, Ratio 20.21x\n",
      "  Fine-tuning model after conv2 compression...\n",
      "  conv2: 0.650 energy, 50.38% val accuracy, 5.76x compression\n",
      "  Approaching minimum accuracy threshold, stopping compression\n",
      "\n",
      "========================================\n",
      "FINAL RESULTS (Validation Set):\n",
      "Original parameters: 1,755,876\n",
      "Compressed parameters: 365,199\n",
      "Overall compression ratio: 4.81x\n",
      "Final validation accuracy: 51.00%\n",
      "Validation accuracy drop: 0.46%\n",
      "Fixed compressions applied: conv1=95%, fc3=90%\n",
      "========================================\n",
      "Final test accuracy for this compression: 52.43%\n",
      "\n",
      "================================================================================\n",
      "TESTING ACCURACY DROP THRESHOLD: 3.0%\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ADAPTIVE COMPRESSION - Max Accuracy Drop: 3.0%\n",
      "============================================================\n",
      "Original validation accuracy: 50.90%\n",
      "Minimum acceptable validation accuracy: 47.90%\n",
      "Using MODERATE compression strategy\n",
      "\n",
      "Applying fixed compressions:\n",
      "  conv1: 95% energy retention\n",
      "  fc3: 90% energy retention\n",
      "  conv1 compressed with 95% energy retention\n",
      "  fc3 compressed with 90% energy retention\n",
      "  Fine-tuning model after fixed compressions...\n",
      "  Accuracy after fixed compressions: 53.32%\n",
      "\n",
      "Compressing conv4...\n",
      "  Current model validation accuracy: 53.32%\n",
      "  Finding optimal compression for conv4...\n",
      "    Coarse search - Retention 0.99: Accuracy 52.82%, Ratio 1.03x\n",
      "    Coarse search - Retention 0.95: Accuracy 53.50%, Ratio 1.32x\n",
      "    Coarse search - Retention 0.90: Accuracy 52.56%, Ratio 1.64x\n",
      "    Coarse search - Retention 0.85: Accuracy 52.76%, Ratio 1.97x\n",
      "    Coarse search - Retention 0.80: Accuracy 52.38%, Ratio 2.34x\n",
      "    Coarse search - Retention 0.75: Accuracy 52.20%, Ratio 2.75x\n",
      "    Coarse search - Retention 0.70: Accuracy 52.40%, Ratio 3.24x\n",
      "    Coarse search - Retention 0.65: Accuracy 51.48%, Ratio 3.84x\n",
      "    Coarse search - Retention 0.60: Accuracy 51.74%, Ratio 4.54x\n",
      "    Coarse search - Retention 0.55: Accuracy 52.10%, Ratio 5.43x\n",
      "    Coarse search - Retention 0.50: Accuracy 50.72%, Ratio 6.63x\n",
      "    Coarse search - Retention 0.45: Accuracy 50.88%, Ratio 8.03x\n",
      "    Coarse search - Retention 0.40: Accuracy 48.80%, Ratio 10.17x\n",
      "    Coarse search - Retention 0.35: Accuracy 48.96%, Ratio 13.02x\n",
      "    Coarse search - Retention 0.30: Accuracy 48.42%, Ratio 17.32x\n",
      "  Fine-tuning model after conv4 compression...\n",
      "  conv4: 0.300 energy, 49.16% val accuracy, 17.32x compression\n",
      "\n",
      "Compressing conv3...\n",
      "  Current model validation accuracy: 49.16%\n",
      "  Finding optimal compression for conv3...\n",
      "    Coarse search - Retention 0.99: Accuracy 49.84%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 49.40%, Ratio 1.37x\n",
      "    Coarse search - Retention 0.90: Accuracy 50.22%, Ratio 1.95x\n",
      "    Coarse search - Retention 0.85: Accuracy 50.06%, Ratio 2.49x\n",
      "    Coarse search - Retention 0.80: Accuracy 50.40%, Ratio 3.07x\n",
      "    Coarse search - Retention 0.75: Accuracy 49.56%, Ratio 3.73x\n",
      "    Coarse search - Retention 0.70: Accuracy 49.04%, Ratio 4.44x\n",
      "    Coarse search - Retention 0.65: Accuracy 48.58%, Ratio 5.35x\n",
      "    Coarse search - Retention 0.60: Accuracy 47.96%, Ratio 6.51x\n",
      "    Coarse search - Retention 0.55: Accuracy 47.72%, Ratio 7.71x\n",
      "    Coarse search - Retention 0.50: Accuracy 47.12%, Ratio 9.44x\n",
      "    Coarse search - Retention 0.45: Accuracy 46.20%, Ratio 11.52x\n",
      "    Coarse search - Retention 0.40: Accuracy 44.64%, Ratio 14.77x\n",
      "    Coarse search - Retention 0.35: Accuracy 42.32%, Ratio 18.73x\n",
      "    Coarse search - Retention 0.30: Accuracy 38.94%, Ratio 22.81x\n",
      "  Fine-tuning model after conv3 compression...\n",
      "  conv3: 0.600 energy, 50.56% val accuracy, 6.51x compression\n",
      "\n",
      "Compressing conv2...\n",
      "  Current model validation accuracy: 50.56%\n",
      "  Finding optimal compression for conv2...\n",
      "    Coarse search - Retention 0.99: Accuracy 49.74%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 50.46%, Ratio 1.36x\n",
      "    Coarse search - Retention 0.90: Accuracy 50.02%, Ratio 2.05x\n",
      "    Coarse search - Retention 0.85: Accuracy 49.70%, Ratio 2.74x\n",
      "    Coarse search - Retention 0.80: Accuracy 50.00%, Ratio 3.36x\n",
      "    Coarse search - Retention 0.75: Accuracy 49.88%, Ratio 4.00x\n",
      "    Coarse search - Retention 0.70: Accuracy 49.98%, Ratio 4.72x\n",
      "    Coarse search - Retention 0.65: Accuracy 48.60%, Ratio 5.76x\n",
      "    Coarse search - Retention 0.60: Accuracy 47.64%, Ratio 6.90x\n",
      "    Coarse search - Retention 0.55: Accuracy 46.14%, Ratio 7.94x\n",
      "    Coarse search - Retention 0.50: Accuracy 46.48%, Ratio 9.37x\n",
      "    Coarse search - Retention 0.45: Accuracy 44.06%, Ratio 11.41x\n",
      "    Coarse search - Retention 0.40: Accuracy 42.70%, Ratio 12.80x\n",
      "    Coarse search - Retention 0.35: Accuracy 38.14%, Ratio 16.94x\n",
      "    Coarse search - Retention 0.30: Accuracy 33.68%, Ratio 20.21x\n",
      "  Fine-tuning model after conv2 compression...\n",
      "  conv2: 0.650 energy, 49.92% val accuracy, 5.76x compression\n",
      "\n",
      "========================================\n",
      "FINAL RESULTS (Validation Set):\n",
      "Original parameters: 1,755,876\n",
      "Compressed parameters: 276,495\n",
      "Overall compression ratio: 6.35x\n",
      "Final validation accuracy: 48.72%\n",
      "Validation accuracy drop: 2.18%\n",
      "Fixed compressions applied: conv1=95%, fc3=90%\n",
      "========================================\n",
      "Final test accuracy for this compression: 51.15%\n",
      "\n",
      "================================================================================\n",
      "TESTING ACCURACY DROP THRESHOLD: 4.0%\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ADAPTIVE COMPRESSION - Max Accuracy Drop: 4.0%\n",
      "============================================================\n",
      "Original validation accuracy: 50.78%\n",
      "Minimum acceptable validation accuracy: 46.78%\n",
      "Using AGGRESSIVE compression strategy\n",
      "\n",
      "Applying fixed compressions:\n",
      "  conv1: 95% energy retention\n",
      "  fc3: 90% energy retention\n",
      "  conv1 compressed with 95% energy retention\n",
      "  fc3 compressed with 90% energy retention\n",
      "  Fine-tuning model after fixed compressions...\n",
      "  Accuracy after fixed compressions: 53.34%\n",
      "\n",
      "Compressing conv4...\n",
      "  Current model validation accuracy: 53.34%\n",
      "  Finding optimal compression for conv4...\n",
      "    Coarse search - Retention 0.99: Accuracy 53.26%, Ratio 1.03x\n",
      "    Coarse search - Retention 0.95: Accuracy 53.24%, Ratio 1.32x\n",
      "    Coarse search - Retention 0.90: Accuracy 53.90%, Ratio 1.64x\n",
      "    Coarse search - Retention 0.85: Accuracy 53.14%, Ratio 1.97x\n",
      "    Coarse search - Retention 0.80: Accuracy 52.76%, Ratio 2.34x\n",
      "    Coarse search - Retention 0.75: Accuracy 52.68%, Ratio 2.75x\n",
      "    Coarse search - Retention 0.70: Accuracy 52.58%, Ratio 3.24x\n",
      "    Coarse search - Retention 0.65: Accuracy 52.58%, Ratio 3.84x\n",
      "    Coarse search - Retention 0.60: Accuracy 52.04%, Ratio 4.54x\n",
      "    Coarse search - Retention 0.55: Accuracy 50.86%, Ratio 5.43x\n",
      "    Coarse search - Retention 0.50: Accuracy 51.78%, Ratio 6.63x\n",
      "    Coarse search - Retention 0.45: Accuracy 51.00%, Ratio 8.03x\n",
      "    Coarse search - Retention 0.40: Accuracy 50.12%, Ratio 10.17x\n",
      "    Coarse search - Retention 0.35: Accuracy 48.94%, Ratio 13.02x\n",
      "    Coarse search - Retention 0.30: Accuracy 47.82%, Ratio 17.32x\n",
      "  Fine-tuning model after conv4 compression...\n",
      "  conv4: 0.300 energy, 50.04% val accuracy, 17.32x compression\n",
      "\n",
      "Compressing conv3...\n",
      "  Current model validation accuracy: 50.04%\n",
      "  Finding optimal compression for conv3...\n",
      "    Coarse search - Retention 0.99: Accuracy 50.74%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 50.52%, Ratio 1.37x\n",
      "    Coarse search - Retention 0.90: Accuracy 49.44%, Ratio 1.95x\n",
      "    Coarse search - Retention 0.85: Accuracy 49.46%, Ratio 2.49x\n",
      "    Coarse search - Retention 0.80: Accuracy 50.32%, Ratio 3.07x\n",
      "    Coarse search - Retention 0.75: Accuracy 49.16%, Ratio 3.73x\n",
      "    Coarse search - Retention 0.70: Accuracy 49.02%, Ratio 4.44x\n",
      "    Coarse search - Retention 0.65: Accuracy 48.96%, Ratio 5.35x\n",
      "    Coarse search - Retention 0.60: Accuracy 49.00%, Ratio 6.51x\n",
      "    Coarse search - Retention 0.55: Accuracy 48.20%, Ratio 7.71x\n",
      "    Coarse search - Retention 0.50: Accuracy 47.02%, Ratio 9.44x\n",
      "    Coarse search - Retention 0.45: Accuracy 45.50%, Ratio 11.52x\n",
      "    Coarse search - Retention 0.40: Accuracy 44.90%, Ratio 14.77x\n",
      "    Coarse search - Retention 0.35: Accuracy 42.16%, Ratio 18.73x\n",
      "    Coarse search - Retention 0.30: Accuracy 40.26%, Ratio 22.81x\n",
      "  Fine-tuning model after conv3 compression...\n",
      "  conv3: 0.500 energy, 49.36% val accuracy, 9.44x compression\n",
      "\n",
      "Compressing conv2...\n",
      "  Current model validation accuracy: 49.36%\n",
      "  Finding optimal compression for conv2...\n",
      "    Coarse search - Retention 0.99: Accuracy 49.40%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 49.72%, Ratio 1.36x\n",
      "    Coarse search - Retention 0.90: Accuracy 49.60%, Ratio 2.05x\n",
      "    Coarse search - Retention 0.85: Accuracy 49.70%, Ratio 2.74x\n",
      "    Coarse search - Retention 0.80: Accuracy 48.96%, Ratio 3.36x\n",
      "    Coarse search - Retention 0.75: Accuracy 48.42%, Ratio 4.00x\n",
      "    Coarse search - Retention 0.70: Accuracy 48.18%, Ratio 4.72x\n",
      "    Coarse search - Retention 0.65: Accuracy 47.20%, Ratio 5.76x\n",
      "    Coarse search - Retention 0.60: Accuracy 47.50%, Ratio 6.90x\n",
      "    Coarse search - Retention 0.55: Accuracy 46.82%, Ratio 7.94x\n",
      "    Coarse search - Retention 0.50: Accuracy 45.14%, Ratio 9.37x\n",
      "    Coarse search - Retention 0.45: Accuracy 43.60%, Ratio 11.41x\n",
      "    Coarse search - Retention 0.40: Accuracy 42.52%, Ratio 12.80x\n",
      "    Coarse search - Retention 0.35: Accuracy 37.28%, Ratio 16.94x\n",
      "    Coarse search - Retention 0.30: Accuracy 33.84%, Ratio 20.21x\n",
      "  Fine-tuning model after conv2 compression...\n",
      "  conv2: 0.550 energy, 48.06% val accuracy, 7.94x compression\n",
      "\n",
      "========================================\n",
      "FINAL RESULTS (Validation Set):\n",
      "Original parameters: 1,755,876\n",
      "Compressed parameters: 258,895\n",
      "Overall compression ratio: 6.78x\n",
      "Final validation accuracy: 48.54%\n",
      "Validation accuracy drop: 2.24%\n",
      "Fixed compressions applied: conv1=95%, fc3=90%\n",
      "========================================\n",
      "Final test accuracy for this compression: 49.48%\n",
      "\n",
      "================================================================================\n",
      "TESTING ACCURACY DROP THRESHOLD: 5.0%\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "ADAPTIVE COMPRESSION - Max Accuracy Drop: 5.0%\n",
      "============================================================\n",
      "Original validation accuracy: 51.52%\n",
      "Minimum acceptable validation accuracy: 46.52%\n",
      "Using AGGRESSIVE compression strategy\n",
      "\n",
      "Applying fixed compressions:\n",
      "  conv1: 95% energy retention\n",
      "  fc3: 90% energy retention\n",
      "  conv1 compressed with 95% energy retention\n",
      "  fc3 compressed with 90% energy retention\n",
      "  Fine-tuning model after fixed compressions...\n",
      "  Accuracy after fixed compressions: 52.20%\n",
      "\n",
      "Compressing conv4...\n",
      "  Current model validation accuracy: 52.20%\n",
      "  Finding optimal compression for conv4...\n",
      "    Coarse search - Retention 0.99: Accuracy 53.20%, Ratio 1.03x\n",
      "    Coarse search - Retention 0.95: Accuracy 53.08%, Ratio 1.32x\n",
      "    Coarse search - Retention 0.90: Accuracy 53.24%, Ratio 1.64x\n",
      "    Coarse search - Retention 0.85: Accuracy 53.70%, Ratio 1.97x\n",
      "    Coarse search - Retention 0.80: Accuracy 52.84%, Ratio 2.34x\n",
      "    Coarse search - Retention 0.75: Accuracy 53.64%, Ratio 2.75x\n",
      "    Coarse search - Retention 0.70: Accuracy 53.00%, Ratio 3.24x\n",
      "    Coarse search - Retention 0.65: Accuracy 52.44%, Ratio 3.84x\n",
      "    Coarse search - Retention 0.60: Accuracy 51.54%, Ratio 4.54x\n",
      "    Coarse search - Retention 0.55: Accuracy 51.54%, Ratio 5.43x\n",
      "    Coarse search - Retention 0.50: Accuracy 50.20%, Ratio 6.63x\n",
      "    Coarse search - Retention 0.45: Accuracy 50.86%, Ratio 8.03x\n",
      "    Coarse search - Retention 0.40: Accuracy 50.18%, Ratio 10.17x\n",
      "    Coarse search - Retention 0.35: Accuracy 49.54%, Ratio 13.02x\n",
      "    Coarse search - Retention 0.30: Accuracy 47.52%, Ratio 17.32x\n",
      "  Fine-tuning model after conv4 compression...\n",
      "  conv4: 0.300 energy, 49.86% val accuracy, 17.32x compression\n",
      "\n",
      "Compressing conv3...\n",
      "  Current model validation accuracy: 49.86%\n",
      "  Finding optimal compression for conv3...\n",
      "    Coarse search - Retention 0.99: Accuracy 50.76%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 50.16%, Ratio 1.37x\n",
      "    Coarse search - Retention 0.90: Accuracy 50.88%, Ratio 1.95x\n",
      "    Coarse search - Retention 0.85: Accuracy 49.14%, Ratio 2.49x\n",
      "    Coarse search - Retention 0.80: Accuracy 49.98%, Ratio 3.07x\n",
      "    Coarse search - Retention 0.75: Accuracy 49.86%, Ratio 3.73x\n",
      "    Coarse search - Retention 0.70: Accuracy 49.24%, Ratio 4.44x\n",
      "    Coarse search - Retention 0.65: Accuracy 48.56%, Ratio 5.35x\n",
      "    Coarse search - Retention 0.60: Accuracy 47.92%, Ratio 6.51x\n",
      "    Coarse search - Retention 0.55: Accuracy 46.96%, Ratio 7.71x\n",
      "    Coarse search - Retention 0.50: Accuracy 47.70%, Ratio 9.44x\n",
      "    Coarse search - Retention 0.45: Accuracy 45.80%, Ratio 11.52x\n",
      "    Coarse search - Retention 0.40: Accuracy 43.92%, Ratio 14.77x\n",
      "    Coarse search - Retention 0.35: Accuracy 41.70%, Ratio 18.73x\n",
      "    Coarse search - Retention 0.30: Accuracy 39.86%, Ratio 22.81x\n",
      "  Fine-tuning model after conv3 compression...\n",
      "  conv3: 0.500 energy, 47.70% val accuracy, 9.44x compression\n",
      "\n",
      "Compressing conv2...\n",
      "  Current model validation accuracy: 47.70%\n",
      "  Finding optimal compression for conv2...\n",
      "    Coarse search - Retention 0.99: Accuracy 47.78%, Ratio 0.93x\n",
      "    Coarse search - Retention 0.95: Accuracy 47.80%, Ratio 1.36x\n",
      "    Coarse search - Retention 0.90: Accuracy 46.98%, Ratio 2.05x\n",
      "    Coarse search - Retention 0.85: Accuracy 48.78%, Ratio 2.74x\n",
      "    Coarse search - Retention 0.80: Accuracy 47.52%, Ratio 3.36x\n",
      "    Coarse search - Retention 0.75: Accuracy 47.80%, Ratio 4.00x\n",
      "    Coarse search - Retention 0.70: Accuracy 48.16%, Ratio 4.72x\n",
      "    Coarse search - Retention 0.65: Accuracy 47.74%, Ratio 5.76x\n",
      "    Coarse search - Retention 0.60: Accuracy 46.22%, Ratio 6.90x\n",
      "    Coarse search - Retention 0.55: Accuracy 45.94%, Ratio 7.94x\n",
      "    Coarse search - Retention 0.50: Accuracy 44.52%, Ratio 9.37x\n",
      "    Coarse search - Retention 0.45: Accuracy 41.66%, Ratio 11.41x\n",
      "    Coarse search - Retention 0.40: Accuracy 40.96%, Ratio 12.80x\n",
      "    Coarse search - Retention 0.35: Accuracy 37.02%, Ratio 16.94x\n",
      "    Coarse search - Retention 0.30: Accuracy 33.66%, Ratio 20.21x\n",
      "  Fine-tuning model after conv2 compression...\n",
      "  conv2: 0.650 energy, 48.10% val accuracy, 5.76x compression\n",
      "\n",
      "========================================\n",
      "FINAL RESULTS (Validation Set):\n",
      "Original parameters: 1,755,876\n",
      "Compressed parameters: 262,415\n",
      "Overall compression ratio: 6.69x\n",
      "Final validation accuracy: 48.90%\n",
      "Validation accuracy drop: 2.62%\n",
      "Fixed compressions applied: conv1=95%, fc3=90%\n",
      "========================================\n",
      "Final test accuracy for this compression: 50.29%\n",
      "\n",
      "========================================================================================================================\n",
      "FIXED COMPRESSION EXPERIMENT SUMMARY (No Data Leakage)\n",
      "========================================================================================================================\n",
      "Acc Drop Val Orig Val Final Test Final Val Drop Test Drop    Ratio  Orig Params  Comp Params\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "     0.0    51.06     52.18      53.05    -1.12     -0.59     4.17    1,755,876      421,519\n",
      "     1.0    51.46     51.00      52.43     0.46      0.03     4.81    1,755,876      365,199\n",
      "     3.0    50.90     48.72      51.15     2.18      1.31     6.35    1,755,876      276,495\n",
      "     4.0    50.78     48.54      49.48     2.24      2.98     6.78    1,755,876      258,895\n",
      "     5.0    51.52     48.90      50.29     2.62      2.17     6.69    1,755,876      262,415\n",
      "\n",
      "Baseline test accuracy: 52.46%\n"
     ]
    }
   ],
   "source": [
    "# Run the improved experiments\n",
    "results = run_improved_compression_experiments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
